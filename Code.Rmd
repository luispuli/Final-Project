---
title: "Final Thesis"
author: "Nacho Pulido"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data mining and processing

### Load relevant libraries

Libraries *tidyverse* , *stringr*, *forcats* will be resorted to at data manipulation stage in order to deal with strings, factors and data processing. Additional libraries are required for model construction and evaluation metrics. We will also need *caret* and *MASS* to operate with our set of models and additional packages such as *httr2* or *rvest* in order to scrape some pages and gather together all available information. For further readings one can always resort to the documentation of a specific package!

```{r}
library(tidyverse)
library(lubridate)
library(janitor)
library(stringr)
library(plotly)
library(ggpubr)
library(scales)
library(forcats)
library(caret)
library(readxl)
library(httr2)
library(rvest)
library(xml2)
library(tidytext)
library(MASS)
library(stargazer)
library(ggeffects)
library(pROC)
library(cluster)
library(mclust)
library(forecast)
library(tseries)
```

### Read and clean attributes

Data is read and processed in order to standardize variables and enable merging with ease at future stages. Notice that the regressor accounting for civil rights is a categorical outcome that spans from "1" (best result) to "7" (worst). Mean age was rejected in favour of a median measure to provide a more robust result and estimate values for the midpoint. For the sake of the reader, the next chunks of codes will not be run, but they supply with information on how the backgroung preprocessing was performed.

    gdp_growth <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/API_NY.GDP.PCAP.KD.ZG_DS2_en_csv_v2_4770505.csv",
             skip = 4)  %>% 
      clean_names() %>% 
      pivot_longer(starts_with("x"),
                   names_to = "Year",
                   values_to = "Gdppc_growth") %>% 
      select(-contains(c("indicator"))) %>% 
      mutate(Year = as.numeric(str_remove(Year,"x")))

    pop_growth <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/API_SP.POP.GROW_DS2_en_csv_v2_4770493.csv",
             skip = 4) %>% 
      clean_names() %>% 
      pivot_longer(starts_with("x"),
                   names_to = "Year",
                   values_to = "Pop_growth") %>% 
      select(-contains(c("indicator"))) %>% 
      mutate(Year = as.numeric(str_remove(Year,"x")))

    continent <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/continents-according-to-our-world-in-data.csv") %>%
      select(Code,Entity,Continent)

    school_years <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/mean-years-of-schooling-long-run.csv")
    names(school_years)[4] <- "School_years"

    educ_expend <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/total-government-expenditure-on-education-gdp.csv")
    names(educ_expend)[4] <- "Educ_expend"
    educ_expend

    health <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/life-expectancy-vs-healthcare-expenditure.csv")
    names(health)[c(4,5)] <- c("Life_expec","health_expdpc")
    health <- health %>% 
      select(1:5)

    migration <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/migration.csv") %>% 
      select(contains("Net"), Year, Country) %>% 
      select(c(1,3,4)) %>% 
      clean_names()

    Age <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/median-age.csv") %>% 
      select(c(1,2,3,4))
    names(Age)[4] <- "Median_age"

    marriage <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/marriage-rate-per-1000-inhabitants.csv") %>% 
      select(c(1,3,4), Code)
    names(marriage)[3] <- "marriage_per_1000"

    civil_rights <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/civil-liberties-fh.csv") %>%
      select(c(1,3,4),Code)
    names(civil_rights)[3] <- "civil_rights"
    #Recall 1 is best and 7 is worst

    geography <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/average-latitude-longitude-countries.csv") %>% 
      select(c(2,3,4))

    religion <- read.csv("C:/Users/Asus/OneDrive/Desktop/Data TFM/religion.txt",
             sep = "") %>% 
      select(Country, Feel) %>% 
      rename("relig_feel" = "Feel")

    data_sustain <- read_excel("data_sustain.xlsx")
    data_sustain <- data_sustain %>% 
      rename("ratio_f_m_educ" = sdg5_edat ,
             "tert_educ" = sdg4_tertiary ,
             "Code" = id ) 

### Webscraping

To work with our PISA data, we had to scrape the page and operate with the resulting HTML. We read the original link from the internet and parse it into an HTML such that we will read the 7th table from the webpage. Beware that such a webpage might be altered in a future, hence this could be another table. We transform this information into a dataframe, clean some attributes and filter for duplicated observations until we have our resulting data to be merged with the rest of databases with ease. Recall this refers to PISA observations on science exams, exclusively.

```{r}
link <- "https://en.wikipedia.org/wiki/Programme_for_International_Student_Assessment"
html_pisa <- link %>% 
  read_html()
table <- html_pisa %>% 
  html_table()
pisa <- table[7] %>% 
  as.data.frame()
pisa <- pisa %>% 
  slice(-c(1,2,3))
names(pisa)[2:9]<- c(rep("2015",2),rep("2012",2),rep("2009",2),rep("2006",2))
pisa <- pisa %>% 
  pivot_longer(-1,names_to = "Year",values_to = "Pisa") %>% 
  rename(Country = Science) %>% 
  group_by(Country) %>% 
  filter(!duplicated(Year)) %>% 
  mutate(Pisa = str_replace(Pisa,"—",as.character(NA))) %>% 
  mutate(Year = as.numeric(Year))
pisa
```

### Merging datasets

We take advantage of country names and years as links to *left join* our dataframes into a single R object. The final csv file is attached to the project. This is a long process oh joining all databases and standardising column names to have our data ready for manipulation and filtering.

    df <- gdp_growth %>% 
      left_join(pop_growth,
                by = c("country_name","Year","country_code")) %>% 
      left_join(continent,
                by = c("country_name" = "Entity","country_code" = "Code" )) %>% 
      left_join(school_years,
                by = c("country_name" = "Entity","Year","country_code" = "Code")) %>%
      left_join(educ_expend,
                by = c("country_name" = "Entity","Year","country_code" = "Code")) %>% 
      left_join(health,
               by = c("country_name" = "Entity","Year","country_code" = "Code")) %>% 
      left_join(migration,
                by = c("country_name" = "country","Year" = "year")) %>%
      left_join(Age,
                by = c("country_name" = "Entity","Year","country_code" = "Code")) %>% 
      left_join(marriage,
                by = c("country_name" = "Entity","Year","country_code" = "Code")) %>% 
      left_join(civil_rights,
                by = c("country_name" = "Entity","Year","country_code" = "Code"))%>% 
      left_join(geography,
                by = c("country_name" = "Country"))%>% 
      left_join(religion,
                by = c("country_name" = "Country"))%>% 
      left_join(data_sustain,
                by = c("country_name" = "Country","country_code" = "Code","Year")) %>% left_join(pisa,
                by = c("country_name" = "Country","Year"))
    write.csv(df,"Country_data.csv")

## Descriptive analysis

We open our resultant csv file to begin with our descriptive analysis. This will be key in order to understand any relationship between our attributes and to have a first approach on the impact of education on population growth rate. Our point of departure will be to read the data and build a world map accounting for differences in median age observed. We will manually add observations for the UK and USA since names were differing when working with the function *map_data*. Of course, the detailed explanations and interpretations of such operations can be read by the user in the pdf version of the project.

```{r}
df <- read.csv("Country_data.csv")
```

Let´s map our data:

```{r}
df$country_name[df$country_name == "United States"] <- "USA"
df$country_name[df$country_name == "United Kingdom"] <- "UK"
df %>% 
  dplyr::select(country_name) %>% 
  filter(str_detect(country_name,"Unit"))
map <- map_data("world") %>% 
  full_join(df, by =  c("region" = "country_name")) 
  
map$Median_age[map$region == "Russia"] <- 38.8

map %>% 
  ggplot(aes(long,lat))+
  geom_polygon(aes( group = group, fill = Median_age))+
  theme_void()+
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())+
  labs(title = "An Ageing World Population", fill = "Median Age")+
  scale_fill_gradient(low='yellow', high= "red" )
```

Next, we will plot the relationship between population growth rate and schooling years as a first approach to notice any existing trends (negative) between education and population dynamics. We will treat years as factors since we will filter for snapshots spanning from 1975 to 2015 and include a regression line to enhance the relationship. Facets will be used to visually inspect this behaviour in each period...

```{r}
df %>% 
  drop_na(Pop_growth,School_years) %>% 
  filter(Year %in% c(1975,1985,2005,2015)) %>% 
  ggplot()+
  aes(Pop_growth,School_years, fill = as.factor(Year))+
  geom_point(shape = 21,alpha = 0.4, color = "white", size = 3)+
  geom_smooth(method = "lm", alpha = 0.4, aes(color = as.factor(Year)))+
  facet_wrap(~as.factor(Year), scales = "free")+
  theme_dark()+
  theme(panel.grid = element_blank(),
        legend.position = "none",
        strip.text = element_text(face = "bold"),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Education level vs Population Growth rate", y = "School Years", x = "Population Growth Rate")
```

Of course, this project wouldn´t make sense without having a previous understanding of the behaviour of population in each continent during the last decades. Let´s plot the evolution of population growth rate by continent by computing the mean rate observed in each of the previously defined years and faceting, this time, by continent:

```{r}
mean_growth <- df %>% 
  drop_na(Continent) %>% 
  filter(Year %in% c(1975,1980,1985,1995,2005,2015)) %>%
  group_by(Continent,Year) %>% 
  summarise(var = mean(Pop_growth)) %>% 
  dplyr::select(Continent, Year, var) %>% 
  ungroup()

mean_growth %>% 
  ggplot()+
  aes(as.factor(Year),var)+
  geom_point(shape = 21, color = "white", size = 3, aes(fill = Continent))+
  geom_line(group = 1, alpha = 0.4, size = 2, color = "white")+
  facet_wrap(~Continent, scales = "free_x")+
  theme_dark()+
  theme(panel.grid = element_blank(),
        legend.position = "none",
        axis.text.x = element_text(angle = 90),
        strip.text = element_text(face = "bold", size = 10),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Average Population Growth", x = "Time", y = " ")+
  scale_y_continuous(labels = label_number(suffix = "%"))

```

The user already has a lot of information about the target variable, the current framework by continent and its relationship with school years. Another interestinf measure would be to account for the relationship between such a growth rate with median years. This will enable us to understand how an ageing population structure can influence the rate at which a population expands, and we will perform, again, a continental exploration:

```{r}
df %>% 
  drop_na(Pop_growth,Median_age, Life_expec) %>% 
  filter(Year %in% c(1975,1985,2005,2015)) %>% 
  ggplot()+
  aes(Median_age,Pop_growth, fill = Continent)+
  geom_point(shape = 21,alpha = 0.4, color = "white", size = 3)+
  geom_smooth(method = "lm",alpha = 0.4, aes(color = Continent))+
  facet_wrap(~Continent, scales = "free")+
  theme_dark()+
  theme(panel.grid = element_blank(),
        legend.position = "none",
        strip.text = element_text(face = "bold"),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Median Age vs Population Growth rate", y = "Population Growth", x = "Median Age")
```

Is education a good indicator of academic performance? We will answer this question by constructing a new variable measuring the interaction between education expenditure and number of school years and evaluate this impact on PISA results. Notice only a small number of countries are subject to such tests, so we have much less observations for these, but we still observe a quite stron relationship:

```{r}
df %>% 
  mutate(inter = Educ_expend*School_years) %>% 
  ggplot()+
  aes(inter,Pisa)+
  geom_point(shape = 21, size = 4, alpha = 0.4, color = "white", fill = rgb(0,0.4,1))+
  theme_classic()+
  geom_smooth(method = "lm")+
  theme_dark()+
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))+
  labs(x = "School Years * Education Investment", y = "Pisa Outcome", title = "Education Returns on Capital Investment")+
  coord_cartesian(xlim = c(5,100), ylim = c(300,600))
  
```

One could have an interest in exploring the relationship between our civil rights variable, the magnitude of religious significance the economy is experiencing and the percentage of women in tertiary education as a ratio to men. It is important to add variables measuring women performance and inclusion because we want to study effects on population, tightly linked to fertility rates, so women are the main characters in this analysis:

```{r}
df %>% 
  group_by(civil_rights) %>% 
  summarise(female_ratio = mean(ratio_f_m_educ,na.rm = TRUE),
            Religion = mean(relig_feel, na.rm = TRUE)) %>% 
  drop_na(civil_rights) %>% 
  ggplot()+
  aes(civil_rights,female_ratio, fill = Religion)+
  geom_bar(stat = "identity", color = "black")+
  coord_cartesian(ylim = c(65,100))+
  theme_dark()+
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Civil rights, Religion and Female Education", y = "Female to Male Ratio", x = "Civil Rights")
```

Let´s explore the relationship between migration rate and population growth. We observe a positive trend (as one could expect) portraying some heteroscedasticity for extreme migration observations:

```{r}
df %>% 
  drop_na(Continent) %>%  
  filter(Pop_growth < 10, Pop_growth> -7) %>% 
  filter(net_migration_rate < 75, net_migration_rate > -50) %>% 
  ggplot()+
  aes(net_migration_rate,Pop_growth)+
  geom_point(shape = 21, color = "white", alpha = 0.3, fill = "blue", size = 4)+
  geom_smooth(color = "black")+
  theme_dark()+
 theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))
```

## Feature engineering

Now we have complete information for each of our economies and by year. However, we will have to perform many transformations and devote some time for inspection and feature engineering if we want to operate with this data (We have NA´s, categorical variables, years without information etc).

### Interaction terms

We will begin by performing an ANOVA test for the interaction term between continent and civil rights on population growth. These variables,both by themseleves and interacted, seem to be relevant in explaining the behaviour of population growth rate since we observe a p-value almost negligible. In an ANOVA test, if the between variation is greater than the within variation in each group, we reject the null and consider some statistical differences. This means we will have to include such interaction term at the modelling stage.

```{r}
data <- df
my_anova <- aov(Pop_growth ~ Continent*civil_rights, data)
summary(my_anova)
```

Let´s explore the effect of interaction terms between our attributes and school years. To do this, we will select numeric variables and generate a new dataframe in which we will apply a function which crosses all columns with schooling years. Next, we will calculate the correlation between such variable and population growth for all variables and append this information at the end in a single data frame:

```{r}
data_new <- 
  data %>% 
  dplyr::select(where(is.numeric)) %>% 
           dplyr::select(-c(1,2,4,5))
inter <- data_new %>% 
  mutate(across(everything(),function(x) x*data$School_years))
inter$Pop_growth <- data$Pop_growth
a <- inter %>% 
  drop_na(Pop_growth,Educ_expend) %>% 
  dplyr::select(Pop_growth,Educ_expend) %>% 
  cor()
b <- inter %>% 
  drop_na(Pop_growth,Life_expec) %>% 
  dplyr::select(Pop_growth,Life_expec) %>% 
  cor()
c <- inter %>% 
  drop_na(Pop_growth,Pisa) %>% 
  dplyr::select(Pop_growth,Pisa) %>% 
  cor()
d <- inter %>% 
  drop_na(Pop_growth,health_expdpc) %>% 
  dplyr::select(Pop_growth,health_expdpc) %>% 
  cor()
f <- inter %>% 
  drop_na(Pop_growth,net_migration_rate) %>% 
  dplyr::select(Pop_growth,net_migration_rate) %>% 
  cor()
g <- inter %>% 
  drop_na(Pop_growth,Median_age) %>% 
  dplyr::select(Pop_growth,Median_age) %>% 
  cor()
h <- inter %>% 
  drop_na(Pop_growth,marriage_per_1000) %>% 
  dplyr::select(Pop_growth,marriage_per_1000) %>% 
  cor()
i  <- inter %>% 
  drop_na(Pop_growth,ratio_f_m_educ) %>% 
  dplyr::select(Pop_growth,ratio_f_m_educ) %>% 
  cor()
j <- inter %>% 
  drop_na(Pop_growth,tert_educ) %>% 
  dplyr::select(Pop_growth,tert_educ) %>% 
  cor()
k <- inter %>% 
  drop_na(Pop_growth,Gdppc_growth) %>% 
  dplyr::select(Pop_growth,Gdppc_growth) %>% 
  cor()
correlation_inter <- rbind(a,b,c,d,f,g,h,i,j,k) %>% 
  as.data.frame() %>% 
  dplyr::select(Pop_growth) %>% 
  rename(Correlation_inter = Pop_growth) %>% 
  slice(seq(2,20,2)) 
correlation_inter <- correlation_inter %>% 
  mutate(Variable = rownames(correlation_inter))
```

We enter this information into ggplot and build our correlation plots in terms of interactions with school years. This will be a tool for us to enable the comparison in terms of correlation between the variable by itself and crossing it with schooling years:

```{r}
correlation_inter %>% 
  ggplot() +
  aes(Correlation_inter,reorder(Variable,Correlation_inter))+
  geom_bar(stat = "identity", color = "black", fill = "red")+
  theme_dark()+
  theme(panel.grid = element_blank())+
  labs(title = "Interaction with School Years",
       y = "Interaction Terms",
       x = "Correlations")
```

### Correlative analysis, Power transformations

Let´s perform the analogous analysis for the amount of correlation of each variable by itself with the target variable:

```{r}
a <- data %>% 
  drop_na(Pop_growth,Educ_expend) %>% 
  dplyr::select(Pop_growth,Educ_expend) %>% 
  cor()
b <- data %>% 
  drop_na(Pop_growth,Life_expec) %>% 
  dplyr::select(Pop_growth,Life_expec) %>% 
  cor()
c <- data %>% 
  drop_na(Pop_growth,Pisa) %>% 
  dplyr::select(Pop_growth,Pisa) %>% 
  cor()
d <- data %>% 
  drop_na(Pop_growth,health_expdpc) %>% 
  dplyr::select(Pop_growth,health_expdpc) %>% 
  cor()
e <- data %>% 
  drop_na(Pop_growth,School_years) %>% 
  dplyr::select(Pop_growth,School_years) %>% 
  cor()
f <- data %>% 
  drop_na(Pop_growth,net_migration_rate) %>% 
  dplyr::select(Pop_growth,net_migration_rate) %>% 
  cor()
g <- data %>% 
  drop_na(Pop_growth,Median_age) %>% 
  dplyr::select(Pop_growth,Median_age) %>% 
  cor()
h <- data %>% 
  drop_na(Pop_growth,marriage_per_1000) %>% 
  dplyr::select(Pop_growth,marriage_per_1000) %>% 
  cor()
i  <-data %>% 
  drop_na(Pop_growth,ratio_f_m_educ) %>% 
  dplyr::select(Pop_growth,ratio_f_m_educ) %>% 
  cor()
j <-data %>% 
  drop_na(Pop_growth,tert_educ) %>% 
  dplyr::select(Pop_growth,tert_educ) %>% 
  cor()
k <- data %>% 
  drop_na(Pop_growth,Gdppc_growth) %>% 
  dplyr::select(Pop_growth,Gdppc_growth) %>% 
  cor()
correlation <- rbind(a,b,c,d,e,f,g,h,i,j,k) %>% 
  as.data.frame() %>% 
  dplyr::select(Pop_growth) %>% 
  rename(Correlation = Pop_growth) %>% 
  slice(seq(2,22,2)) 
correlation <- correlation %>% 
  mutate(Variable = rownames(correlation))
```

We pipe this results into *ggplot* and set the identity stat on our geometry. The theme selected will be the same as for our previous plot. We can already notice that those variables holding the highest magnitude of positive correlation with population growth rate are migration rate and marriage whereas the most relevant ones in terms of negative association are median age and school years. The ratio of female to male people in tertiary education hold a very high correlation coefficient of almost -0.4!

```{r}
correlation %>% 
  ggplot() +
  aes(Correlation,reorder(Variable,Correlation))+
  geom_bar(stat = "identity", color = "black", fill = "red")+
  theme_dark()+
  theme(panel.grid = element_blank())+
  labs(y = "Absolute terms", x = "Correlation",
       title = "Absolute Correlation")
```

Let´s perform the same anaylisis for power terms by generating a function which squares the value. This function will be applied to every numeric attribute in our data and we will compute the amount of correlation between each variable and population growth rate:

```{r}
square <- function(x){
  x^2
}
data_sq <- data %>% 
  dplyr::select(where(is.numeric), -c(X,Year,Pop_growth)) %>% 
  mutate(across(everything(),square))
data_sq$Pop_growth <- data$Pop_growth
a <- data_sq %>% 
  drop_na(Pop_growth,Educ_expend) %>% 
  dplyr::select(Pop_growth,Educ_expend) %>% 
  cor()
b <- data_sq %>% 
  drop_na(Pop_growth,Life_expec) %>% 
  dplyr::select(Pop_growth,Life_expec) %>% 
  cor()
c <- data_sq %>% 
  drop_na(Pop_growth,Pisa) %>% 
  dplyr::select(Pop_growth,Pisa) %>% 
  cor()
d <- data_sq %>% 
  drop_na(Pop_growth,health_expdpc) %>% 
  dplyr::select(Pop_growth,health_expdpc) %>% 
  cor()
e <- data_sq %>% 
  drop_na(Pop_growth,School_years) %>% 
  dplyr::select(Pop_growth,School_years) %>% 
  cor()
f <- data_sq %>% 
  drop_na(Pop_growth,net_migration_rate) %>% 
  dplyr::select(Pop_growth,net_migration_rate) %>% 
  cor()
g <- data_sq %>% 
  drop_na(Pop_growth,Median_age) %>% 
  dplyr::select(Pop_growth,Median_age) %>% 
  cor()
h <- data_sq %>% 
  drop_na(Pop_growth,marriage_per_1000) %>% 
  dplyr::select(Pop_growth,marriage_per_1000) %>% 
  cor()
i  <-data_sq %>% 
  drop_na(Pop_growth,ratio_f_m_educ) %>% 
  dplyr::select(Pop_growth,ratio_f_m_educ) %>% 
  cor()
j <-data_sq %>% 
  drop_na(Pop_growth,tert_educ) %>% 
  dplyr::select(Pop_growth,tert_educ) %>% 
  cor()
k <- data_sq %>% 
  drop_na(Pop_growth,Gdppc_growth) %>% 
  dplyr::select(Pop_growth,Gdppc_growth) %>% 
  cor()
correlation_sq <- rbind(a,b,c,d,e,f,g,h,i,j,k) %>% 
  as.data.frame() %>% 
  dplyr::select(Pop_growth) %>% 
  rename(Correlation_sq = Pop_growth) %>% 
  slice(seq(2,22,2)) 
correlation_sq <- correlation_sq %>% 
  mutate(Variable = rownames(correlation_sq))
```

As usual, we enter this information into ggplot:

```{r}
correlation_sq %>% 
  ggplot() +
  aes(Correlation_sq,reorder(Variable,Correlation_sq))+
  geom_bar(stat = "identity", color = "black", fill = "red")+
  theme_dark()+
  theme(panel.grid = element_blank())+
  labs(y = "Power terms", 
       x = "Correlation",
       title = "Correlation with Power Terms")
```

### Visualizing the Matrix

By comparing the magnitude of these correlations, we have determined that it is imperative to include a squared term for both life expectancy and marriage in combination with joint effects for education expenditure, life expectancy, and female to male ratio on tertiary education. Our study also calls for logarithmic features for many of our variables, which is a common technique in economics when accounting for growth rates and for the sake of avoiding heteroscedastic issues.

```{r}
fet_engin <- cbind(correlation,correlation_sq) %>% 
  dplyr::select(-2) %>% 
  left_join(correlation_inter, by = "Variable") %>% 
  dplyr::select(Variable,Correlation,Correlation_sq,Correlation_inter)

fet_engin %>% 
  filter(abs(Correlation_sq)>abs(Correlation))

#square term on life expectancy and marriage relevant

fet_engin %>% 
  filter(abs(Correlation_inter)>abs(Correlation))
```

## Multiple Imputation

One of our main concerns at the data manipulation and encoding stage was the substantial number of missing values stemming from database merging, time measurement inconsistencies or absence of full public disclosure in some cases. Let´s generate two dataframes computing the number of missing observations in each column for data previous 1980 and beyond. To do this, we will use the *is.na* operator, add the results for each vector and divide the outcome by the total number of rows in our data:

```{r}
df <- data
missing_1 <- df %>% 
  is.na() %>% 
  colSums() %>% 
  as.data.frame() %>% 
  rename(Missing = ".") %>% 
  mutate(Percentage = Missing/nrow(df),
         Attribute = as.vector(names(df)))

```

We operate analogously by filtering for data starting in 1980:

```{r}
missing_2 <- df %>% 
  filter(Year > 1979) %>% 
  is.na() %>% 
  colSums() %>% 
  as.data.frame() %>% 
  rename(Missing = ".") %>% 
  mutate(Percentage = Missing/nrow(df),
         Attribute = as.vector(names(df))) 
```

We will build a slightly more advanced plot in which we will attach one graph in front of the other to visually explore the differences in missing observations with and without filtering. This illustration is an advocate for the fundamental decision of restricting the sample period to the one proposed:

```{r}
missing_1 %>% 
  ggplot()+
  aes(Percentage, reorder(Attribute,Percentage))+
  geom_bar(stat = "identity", color = "white", fill = "red")+
  geom_bar(aes(Percentage, Attribute),data = missing_2, fill = "Blue", stat = "identity", color = "white")+
  labs(x = "Proportion of Missing values", title = "Filtering for later than 1980", y = "Proportion of Missing Values")
```

Hence, let´s filter the results from now on from 1980 onwards:

```{r}
df <- 
  df %>% 
  filter(Year > 1979)
```

### School Years

The choice of the best fitting imputation technique is a sound approach based on the visualization of the distribution before and after imputation which enables the analyst to discriminate within algorithms and establish which technique preserves the original characteristics of the data. We will impute missing observations for some of our variables (the ones with more percentage of missing observations) and select the algorithm yielding the best outcome. We begin with school years (please, notice this takes a while):

```{r}
library(mice)
mice_imputed <- data.frame(
  original = df$School_years,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$School_years,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$School_years,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$School_years
)
```

In this case, the distribution selected is the output generated from the pmm distribution. Predictive Mean Matching (PMM) will regress the variable holding missing observations against the rest of attributes and compute the most plausible measurement to replace the empty field whereas classification and regression trees (CART) have decision nodes

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of School Years") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

As a final step, we replace these values in the original dataframe:

```{r}
df$School_years <- mice_imputed$imputed_pmm
```

### Life Expectancy

We perform the same for the variable on life expectancy...

```{r}

mice_imputed <- data.frame(
  original = df$Life_expec,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$Life_expec,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$Life_expec,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$Life_expec
)
```

Visually inspect the most appropriate distribution:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Life Expectancy") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

Replace the original values in the corresponding data column:

```{r}
df$Life_expec <- mice_imputed$imputed_pmm
```

### Net Migration Rate

Same for net migration rate:

```{r}
mice_imputed <- data.frame(
  original = df$net_migration_rate,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$net_migration_rate,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$net_migration_rate,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$net_migration_rate
)
```

Again, compare the different distributions obtained:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Migration Rate") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

Once we have selected the best imputation algorithm, we replace the original data with it. Notice that, in this case, we are resorting to the "cart" procedure.

```{r}
df$net_migration_rate <- mice_imputed$imputed_cart
```

### Female to Male Ratio

For the case of female to male ratio in tertiary education:

```{r}
mice_imputed <- data.frame(
  original = df$ratio_f_m_educ,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$ratio_f_m_educ,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$ratio_f_m_educ,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$ratio_f_m_educ
)
```

We plot the results arising from the different algorithms:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution Ratio Female to Male") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

Replace our results in the original dataframe we will perform the same for the rest of variables, this might get a bit boring for the reader, one can always skip this and directly move to the next section in which we gather all these steps together in a single csv!

```{r}
df$ratio_f_m_educ <- mice_imputed$imputed_cart
```

### Marriage

Marriage is imputed:

```{r}
mice_imputed <- data.frame(
  original = df$marriage_per_1000,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$marriage_per_1000,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$marriage_per_1000,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$marriage_per_1000
)
```

The imputed distributions are plotted and visually compared:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Marriage Rate") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

```{r}
df$marriage_per_1000 <- mice_imputed$imputed_pmm
```

### Education Expenditure

Same for education expenditure...

```{r}
mice_imputed <- data.frame(
  original = df$Educ_expend,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$Educ_expend,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$Educ_expend,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$Educ_expend
)
```

We plot the imputed results of education expenditure. This variable was a bit more difficult to impute because it had great variability and portrayed some outliers:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Education Expenditure") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

```{r}
df$Educ_expend <- mice_imputed$imputed_pmm
```

### Health Expenditure

We impute in the same way our health dimension (expenditure):

```{r}
mice_imputed <- data.frame(
  original = df$health_expdpc,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$health_expdpc,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$health_expdpc,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$health_expdpc
)
```

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Public Health Expenditure") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

This was one of the cases in which we preferred the "cart" algorithm.

```{r}
df$health_expdpc <- mice_imputed$imputed_cart
```

### Median Age

Median age was one of the variables with fewer proportion of missing observations, but we will manage this values anyway:

```{r}
library(mice)
mice_imputed <- data.frame(
  original = df$Median_age,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$Median_age,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$Median_age,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$Median_age
)
```

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Median Age") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

Replace these results in the original dataframe:

```{r}
df$Median_age <- mice_imputed$imputed_cart
```

### Tertiary Education

On the other hand, the percentage of people in tertiary education had one of the greatest percentage of missing observations, which calls for the introduction of our imputation techniques:

```{r}
mice_imputed <- data.frame(
  original = df$tert_educ,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$tert_educ,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$tert_educ,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$tert_educ
)
```

We analyse carefully each of the distributions:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Tertiary Education %") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

```{r}
df$tert_educ <- mice_imputed$imputed_pmm
```

### Civil Rights

We operate the same with civil rights. Recall this variable is defined as a categorical one ranging from 1 to 7 in which 1 is the best and 7 refers to countries with extremely low levels of individual freedom.

```{r}
mice_imputed <- data.frame(
  original = df$civil_rights,
  imputed_pmm = complete(mice(df, m=5, method = "pmm"))$civil_rights,
  imputed_cart = complete(mice(df, m=5, method = "cart"))$civil_rights,
  imputed_lasso = complete(mice(df, m=5, method = "lasso.norm"))$civil_rights
)
```

We compare the distribution of all imputed values:

```{r}
h1 <- ggplot(mice_imputed, aes(x = original)) +
  geom_histogram(binwidth = 1, color = "skyblue3", fill = "skyblue") +
  ggtitle("Distribution of Civil Rights") +
  theme_classic()
h2 <- ggplot(mice_imputed, aes(x = imputed_pmm)) +
  geom_histogram(binwidth = 1, fill = "#15ad4f", color = "#000000", position = "identity") +
  ggtitle("PMM-imputed distribution") +
  theme_classic()
h3 <- ggplot(mice_imputed, aes(x = imputed_cart)) +
  geom_histogram(binwidth = 1, fill = "#1543ad", color = "#000000", position = "identity") +
  ggtitle("Cart-imputed distribution") +
  theme_classic()
h4 <- ggplot(mice_imputed, aes(x = imputed_lasso)) +
  geom_histogram(binwidth = 1, fill = "#ad8415", color = "#000000", position = "identity") +
  ggtitle("Lasso-imputed distribution") +
  theme_classic()

ggarrange(h1,h2,h3,h4)
```

```{r}
df$civil_rights <- mice_imputed$imputed_cart
```

### Missing Continents: Inspection

As an additional section on the imputation stage, some continent attributes were lost (likely due to merging of the different soruces) and we manually recover these values by replacing inside the continent column, in the place where the country name matches with the selected query, the appropriate continent they belong to:

```{r}

df$Continent[df$country_name == "Channel Islands"] <- "Europe"
df$Continent[df$country_name == "Congo, Rep."] <- "Africa"
df$Continent[df$country_name == "Cabo Verde"] <- "Africa"
df$Continent[df$country_name == "Gambia, The"] <- "Africa"
df$Continent[df$country_name == "Korea, Rep."] <- "Asia"
df$Continent[df$country_name == "Slovak Republic"] <- "Europe"

```

We can now save these results inside a new csv:

```{r}
write.csv(df,"Clean_data.csv")
```

## Feature Generation

We will now devote a section to generate all the attributes we developed at the feature engineering stage. This step is important because it will be key to improve the performance of our models and capture non linear trends in the data.

```{r}
data <- read.csv("Clean_data.csv")
```

We first generate the square terms by mutating across life expectancy and marriage and applying our square function.

```{r}
squares <- data %>% 
  mutate(across(c(Life_expec,marriage_per_1000),square)) %>% 
  dplyr::select(Life_expec_sq = Life_expec,marriage_per_1000_sq = marriage_per_1000)
```

For the interaction terms, we will cross each attribute with school years and select the remaining variables. Recall this will be applied to gdp growth rate, education expenditure, life expectancy and our female to male ratio in education.

```{r}
interactions <- 
  data %>% 
  mutate(educexp_schlyrs = Educ_expend*School_years,
         lifexp_schlyrs = Life_expec*School_years,
         ratio_fm_schlyrs = ratio_f_m_educ*School_years,
         gdp_schlyrs = Gdppc_growth*School_years) %>% 
  dplyr::select(educexp_schlyrs,lifexp_schlyrs,ratio_fm_schlyrs,gdp_schlyrs)
```

Our study also calls for logarithmic features for many of our variables, which is a common technique in economics when accounting for growth rates and for the sake of avoiding heteroscedastic issues. In future sections, we plan to perform an automated feature selection method to further refine our model and improve its predictive power.

```{r}
logs <- 
  data %>% 
  mutate(lg_Median_age = log(Median_age),
         lg_health_expdpc = log(health_expdpc),
         lg_Life_expec = log(Life_expec),
         lg_ratio_fm = log(ratio_f_m_educ),
         lg_Educ_expend = log(Educ_expend),
         lg_Gdppc_growth = log(Gdppc_growth+65)) %>% 
  dplyr::select(starts_with("lg"))
```

We append this information together inside the same dataframe and save it.

```{r}
data <- cbind(data,logs,interactions,squares)
```

```{r}
write.csv(data,"final_data.csv")
```

## Lasso regression

At this stage, we will apply our feature selection algorithms in order to discard redundant variables and improve the performance of the model. If we include too many variables, we are likely to incur in multicollinearity issues which can lead to overfitting our model and worsen the performance at the testing stage.

We drop missing observations of population growth rate and remove unnecessary variables such as country name.

```{r}
data <- read.csv("final_data.csv")
data <- data %>% 
  drop_na(Pop_growth)


data <-data %>% 
 dplyr::select(-(starts_with("X")|starts_with("country")),-c(Year,Pisa,Longitude,Latitude))

```

## Winzorization

We had noticed at earlier stages of the project the fact that we have many outliers. With some trial and error, we have determined that by applying some winzorisation techniques and replacing those extreme observations by the 98th and 3rd percentile, the performance of our models is enhanced:

```{r}
lower_bound <- quantile(data$Pop_growth,probs = 0.03)
upper_bound <- quantile(data$Pop_growth,probs = 0.98)

data$Pop_growth[data$Pop_growth > upper_bound] <- upper_bound

data$Pop_growth[data$Pop_growth < lower_bound] <- lower_bound

```

We split our data into training and testing set to perform our machine learning algorithms.

```{r}
set.seed(123)
training_samples <- data$Pop_growth %>% 
  createDataPartition(p = 0.7, list = FALSE)

train_data <- data %>% 
  slice(training_samples)

test_data <- data %>% 
  slice(-training_samples)
```

We will resort to cross validation with 6 folds and set the corresponding hyperparameter for lasso regression. This process is iterated k times (each fold used once for validation) and the performance of each model is evaluated based on our predefined performance metrics until the hyperparameters outputting the best realization are designated for the final process.

```{r}
set.seed(123)
train_data <-
  train_data %>% 
  drop_na()

ctrl <- trainControl(method = "repeatedcv", 
                     number = 6,
                     verboseIter = T)

lasso_grid <- expand.grid(fraction = seq(0.1, 0.5, length = 100))

lasso_tune <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent, data = train_data,
                    method='lasso',
                    preProc=c('scale','center'),
                    tuneGrid = lasso_grid,
                    trControl=ctrl)
best <- as.numeric(lasso_tune$bestTune)

predict(lasso_tune$finalModel, type = "coeff", mode = "fraction", s = best)

```

Let´s plot the results of the optimized Lasso hyperparameter in terms of the R-squared. The R-squared measures the amount of correlation observed between the predicted values and actual observations.

```{r}
regularization <- lasso_tune$results %>% 
  ggplot()+
  aes(fraction,Rsquared)+
  theme_dark()+
  geom_point(fill = "blue", color = "blue", alpha = 0.2)+
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))+
  geom_vline(xintercept = best, color = "white", linetype = 2)+
  labs(title = "Optimum Cross-Validated Regularization",
       x = "Hyperparameter")+
  geom_line()+
  coord_cartesian(ylim = c(0.8,0.823))

regularization

ggsave("regular_plot.png", regularization)


```

## Ridge Regression

Ridge regression is different from Lasso in the sense that the penalization introduced is calculated with the square of the coefficients of the linear model. We train our ridge regression and compare the results with the previous model. We will also explore the best tune of this model:

```{r}
ridge_grid <- expand.grid(lambda = seq(0, .1, length = 100))

ridge_tune <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent , data = train_data,
                    method='ridge',
                    preProc=c('scale','center'),
                    tuneGrid = ridge_grid,
                    trControl=ctrl)
ridge_tune$bestTune

```

## Elastic Net

The parameter λ accounts for the penalty introduced for the inclusion of parameters and this will behave as a hyperparameter such that we will resort to cross-validation to find the optimal solution for this measure. As the parameter increases, the penalty introduced is higher and, therefore, in lasso regression it can happen that some parameters are brought completely to 0 (null). Elastic net will be a combination of the lasso and ridge mechanisms called before.

```{r}
set.seed(123)
elastic_grid = expand.grid(alpha = seq(0, .2, 0.01), lambda = seq(0, .1, 0.01))

glmnet_tune <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent, data = train_data,
                     method='glmnet',
                     tuneGrid = elastic_grid,
                     trControl=ctrl)
glmnet_tune$bestTune
```

```{r}
table <- as.data.frame(as.matrix(predict(glmnet_tune$finalModel, type = "coeff", mode = "lambda", s = 0.1))) %>% 
  mutate(regularized = ifelse(s1 == 0, "Regularized",
                              "Fitting"))

write.csv(table, "elasticnet.txt")
```

### Model Selection

Our criteria will depend both on the R-squared and a qualitative side (more detailed explanations are included in the pdf version of the project):

```{r}
test_data <- 
  test_data %>% 
  drop_na()
preds_lasso <- lasso_tune %>% 
  predict(test_data,type = "raw")
(cor(preds_lasso,test_data$Pop_growth))^2

```

Let´s compute the R-squared for our ridge regression by calculating the correlation between our predictions and actual observations and square this term.

```{r}
preds_ridge <- ridge_tune %>% 
  predict(test_data,type = "raw")
(cor(preds_ridge,test_data$Pop_growth))^2
```

Compute the R-squared value for the elastic net model:

```{r}
preds_glmnet <-  glmnet_tune %>% 
  predict(test_data,type = "raw")
round((cor(preds_glmnet,test_data$Pop_growth))^2,2)
```

## Multiple Linear Model

We run our final multiple linear model based on the variable inclusion from our elastic net models. This will ensure we avoid multicollinear issues and enhance of our predictions. We will also include interaction terms with our continent attribute since this couldn´t have been generated at the feature engineering stage because of the nature of such a variable (categorical labelled). To ease interpretation, the inclusion of a variable both in absolute terms and logarithmic transformation was never introduced.

```{r}
final_model <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent - Life_expec_sq - civil_rights - educexp_schlyrs - Life_expec - lg_ratio_fm -lifexp_schlyrs - marriage_per_1000_sq - lg_Gdppc_growth - lg_Educ_expend - lg_health_expdpc - Median_age, data = train_data,
                     preProc=c('scale','center'),
                     method='lm',
                     trControl=ctrl)


summary(final_model)
```

We develop a residual plot in which the horizontal axis accounts for the fitted values of the model. This plot will reveal any non-linear patterns and point to the presence of heteroscedastic issues. We expect to see a residual plot arising from a normal distribution with 0 mean (sum of residuals should be null) and ressembling a white noise process:

```{r}
res <- resid(final_model)
data.frame(fitted = fitted(final_model), res) %>% 
  ggplot()+
  aes(fitted, res, color = abs(res)>2)+
  geom_point()+
  coord_cartesian(xlim = c(1,3))+
  theme(legend.position = "None")+
  labs(y = "Residual", x = "Fitted", title = "Residual Plot")+
  scale_color_manual(values=c("black", "red"))
```

```{r}
stargazer(final_model$finalModel,
          title = "Multiple Linear Model",
          align = TRUE,
          type = "latex",
          out="table1.doc")
```

We observe how, almost all coefficients are statistically significant in the model yielding an R-squared of around 82%. Let´s use our model to predict the results of our test data:

```{r}
preds_finalmodel <-  final_model %>% 
  predict(test_data,type = "raw")
round((cor(preds_finalmodel,test_data$Pop_growth))^2,2)
```

The following plot displays the fitted values of our linear model, which captures 81% of the variance in the response variable. Cutoff thresholds represented on the graph account for winsorization, which was introduced as an earlier stage to manage the behavior of outliers in a way that top 2 percent deviations were replaced by quantiles 98 and 2, respectively.

```{r}
ggplot(data.frame(test_data$Pop_growth,preds_finalmodel))+
  aes(test_data.Pop_growth,preds_finalmodel)+
  geom_point()+
  theme_dark()+
  geom_point(shape = 21, size = 3.5, fill = "blue", color = "white", alpha = 0.4)+
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Linear Predictions")+
  geom_abline(slope = 1, intercept = 0, color = "black", size = 1, alpha = 0.5)+
  labs(y = "Predicted Outcome", x = "True Value")+
  annotate("text", x = 1.8, y = 2.2, label = "45° Line", color = "White", angle = 17)+
  coord_cartesian(xlim = c(-1,5), ylim = c(-1,5))+
  geom_vline(xintercept = c(lower_bound,upper_bound), color = "white", linetype = 2)
```

## Hypothesis test

The second hypothesis requires the definition of our multiple restriction F-statistic as stated in the following equation in which we will compare the residuals of the restricted model with the unbounded one. One would be interested in determining if the marginal effect of more women in tertiary education and the weight of boosting the aggregate proportion of people tertiary education are symmetrical:

```{r}
restricted_model <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent - Life_expec_sq - civil_rights - educexp_schlyrs - Life_expec - lg_ratio_fm -lifexp_schlyrs - marriage_per_1000_sq + I(ratio_f_m_educ+tert_educ)+I(School_years*(ratio_f_m_educ+tert_educ))-ratio_f_m_educ-tert_educ-ratio_fm_schlyrs, data = train_data,
                     preProc=c('scale','center'),
                     method='lm',
                     trControl=ctrl)
summary(restricted_model)
restricted_model$results
```

We perform the multiple restriction hypothesis test and obtain the p-value with respect to our F-statistic:

$$ F = \frac{{(R^2_{UR} - R^2_R)(n - k - 1)}}{{q(1 - R^2_{UR})}} $$

```{r}
R_restricted <- 0.8199
R_unrestricted <- 0.8205
n <- 3067
k <- 18
q <- 1
f_stat <- ((R_unrestricted - R_restricted)*(n-k-1))/(q*(1-0.8205))
round(1-pf(f_stat,q,n-k-1),3)
paste("The p-value is",round(1-pf(f_stat,q,n-k-1),3),",hence we reject the null")
```

The F-statistic (p\<0.05) lead the way to rejecting the null hypothesis such that the non-restricted model was yielding a better fitting in terms of squared residuals, hence it is established that the marginal effect of these two attributes remain unequal. To test for the one holding the greatest impact in population growth rate, we trained 3 additional models accounting for the absence of these two variables, the inclusion of one of them, and the admission of the other to monitor which one was being the major player in the R-squared of the model.

```{r}
model_1 <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent - Life_expec_sq - civil_rights - educexp_schlyrs - Life_expec - lg_ratio_fm -lifexp_schlyrs - marriage_per_1000_sq - ratio_f_m_educ-tert_educ-ratio_fm_schlyrs, data = train_data,
                     preProc=c('scale','center'),
                     method='lm',
                     trControl=ctrl)
summary(model_1)
R_1 <- 0.8187
```

```{r}
model_2 <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent - Life_expec_sq - civil_rights - educexp_schlyrs - Life_expec - lg_ratio_fm -lifexp_schlyrs - marriage_per_1000_sq - ratio_f_m_educ-ratio_fm_schlyrs, data = train_data,
                     preProc=c('scale','center'),
                     method='lm',
                     trControl=ctrl)
summary(model_2)
R_2 <- 0.819
```

```{r}
model_3 <- train(Pop_growth ~ . + Continent*civil_rights + Continent*School_years - Continent - Life_expec_sq - civil_rights - educexp_schlyrs - Life_expec - lg_ratio_fm -lifexp_schlyrs - marriage_per_1000_sq - tert_educ, data = train_data,
                     preProc=c('scale','center'),
                     method='lm',
                     trControl=ctrl)
summary(model_3)
R_3 <- 0.8202
```

```{r}
data.frame(R_1,R_2,R_3)
```

## Logistic Model

The output coefficients from our previous model are fitted into the generation of a single-weighted education attribute arguing for both the econometric omission of any potential confounders and the significance of each effect on population dynamics. We generate our target variable (positive or negative population growth rate) and the "education dimension" as our weighted average.

```{r}
data_logistic <- data %>% 
  mutate(target = ifelse(Pop_growth > 0,0,1),
         education = 0.21*School_years + 0.08*Educ_expend +
           0.15*ratio_f_m_educ + 0.03*tert_educ + 0.03*gdp_schlyrs + 0.28*ratio_fm_schlyrs) %>% 
  drop_na()

```

We relevel the target variable to calculate the probabilities in terms of the odds of incurring into negative growth rates:

```{r}
data_logistic <- data_logistic %>% 
  mutate(target = as.factor(target),
         target = relevel(target, ref = "0"))
```

As usual, we split our logistic data into the training and testing set:

```{r}
set.seed(123)
training_samples <- data_logistic$target %>% 
  createDataPartition(p = 0.6, list = FALSE)
train_data <- data_logistic %>% 
  slice(training_samples)
test_data <- data_logistic %>% 
  slice(-training_samples)
```

As an initial step, let´s plot the distribution of our education variable (which spans from 0 to less than 500) for our two groups of economies. These are, countries holding negative and positive growth rates in population. We notice the fact that economies which are experiencing negative growth rates are much more likely to have larger values of our education dimension:

```{r}

data_logistic %>% 
  ggplot()+
  aes(education, fill = target)+
  geom_density() +
  facet_grid(~target)+
  theme_dark()+
  theme(panel.grid = element_blank(),
        strip.text = element_blank(),
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5))+
  labs(title = "Education Distribution by Growth Rate",
       fill = "Growth rate", 
       x = "Education",
       y = "Density")+
  scale_fill_discrete(labels=c('Positive', 'Negative'))

```

Since there is an imbalance issue in our data set such that most of our economies hold positive population growth rates, we will resort to the *upSample* function which will create synthetic values based on our observations in order to weight more the minority class and obtain more accurate results in our binary classification model.

```{r}
set.seed(123)
train_data <- upSample(train_data,train_data$target)
```

We train our model as a function of education and the continental dimension. The aim is to be able to understand how much we can predict in terms of population dynamics just by resorting to the education variable. Our hypothesis establishes that education is a sufficiently powerful indicator of population growth rate.

```{r warning=FALSE}
logit <- glm(target ~  Continent + education, data = train_data, family = "binomial")

summary(logit)

```

Let´s plot the marginal effects of the model and obtain a different curve for each of our continents. We close our binary analysis by plotting the results of the marginal effect of our model as a function of our education aggregate and quantify these results by continent. This is, together with the results of the linear model and the hypothesis on the importance of gender parity, one of the most noteworthy findings from the study. One identifies not merely that the partial contribution of education to the likelihood of experiencing a negative population growth rate follows a concave functionality (with leveler marginal effects at the extremes), but additionally these results differ significantly across continents. 

```{r}
gg.pred <- ggpredict(logit, terms = c("education","Continent"))
plot(gg.pred)+
  geom_line()+
  labs(title = "Marginal Effect on Negative Growth",
       x = "Education", y = "Target")+
  theme(plot.title = element_text(hjust = 0.5))
```

For an "education level" of 300, the probability of an African region of holding negative growth rate is of 50 percentage points less than for Europe. This is, Europe and Asia are significantly more at risk than America or Oceania of experiencing these negative rates.

What does this look like in a classified confusion matrix? We will choose the threshold such that we will maximise the sensitivity, this is, the value which accounts for the correct classification of the minority class:

```{r}
probabilities <- logit %>% 
  predict(test_data,type = "response")
predicted_cases <- ifelse(probabilities>0.5, "1", "0")
confusionMatrix(as.factor(predicted_cases), as.factor(test_data$target), positive = "1")
```

One might also be interested in exploring the AUC, which is the metric we will use to evaluate our model. This is the area under the ROC curve, which plots, at different thresholds, the tradeoffs faced between sensitivity and specificity. We obtain an AUC of 0.88, which means that our education dimension is able to predict correctly the population rate status around close to 88% of the times. Notice we haven´t input more variables than the one that we had constructed for education.

```{r}

roc <-roc(as.factor(test_data$target),probabilities)
sens <- roc$sensitivities
spec <- roc$specificities
ggplot(as.data.frame(sens,spec), aes(x = 1-spec,y = sens))+
  geom_line(color = "brown", size = 1)+
  annotate("text", x = 0.5, y = 0.4, label = paste0("AUC = ", round(auc(roc), 2)), size = 5, color = "black", fontface = "bold")+
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = rgb(1,0.8,0.5)),panel.background = element_rect(fill = rgb(1,1,1)))+
  labs(title = "ROC curve", y = "Sensitivity", x = "1 - Specificity")
  
```

## Cluster Analysis

```{r}
data <- read.csv("final_data.csv")
```

By resorting to our education aggregate computed in the former segment as the exclusive input, we will separate our countries into 5 distinct clusters based on the unsupervised k-means algorithm which groups the observations by minimizing the Euclidian distance to each center. Acknowledging that one should ensure observations within a cluster are similar and variability between collections should be maximized, this process will be iterated several times and the resulting combination yielding the minimum variation will be selected. 

```{r}
data_rec <- data %>% 
  filter(Year > 2007) %>% 
  mutate(target = ifelse(Pop_growth > 0,0,1),
         education = 0.21*School_years + 0.08*Educ_expend +
           0.15*ratio_f_m_educ + 0.03*tert_educ + 0.03*gdp_schlyrs + 0.28*ratio_fm_schlyrs) %>% 
  dplyr::select(country_name,education)
head(data_rec)

```

We generate our target variable and select the corresponding attributes required for such analysis:

```{r}
data <- data %>% 
  mutate(target = ifelse(Pop_growth > 0,0,1),
         education = 0.21*School_years + 0.08*Educ_expend +
           0.15*ratio_f_m_educ + 0.03*tert_educ + 0.03*gdp_schlyrs + 0.28*ratio_fm_schlyrs) %>% 
  dplyr::select(country_name,education)
head(data)

```

We evaluate the mean education-aggregate obtained for each country for the given period in order to cluster and repeat the same analysis for the last 10 years of data to notice any relevant changes. This information is introduced in the appendix as a supplement to the core project:

```{r}
data_rec <- data_rec %>% 
  group_by(country_name) %>% 
  summarise(mean_educ = mean(education,na.rm = TRUE)) %>% 
  drop_na()
names_rec <- data_rec$country_name
data_rec$country_name <- NULL
data_rec
```

```{r}
data <- data %>% 
  group_by(country_name) %>% 
  summarise(mean_educ = mean(education,na.rm = TRUE)) %>% 
  drop_na()
names <- data$country_name
data$country_name <- NULL
data
```

```{r}
set.seed(123)
k <- 5
model_rec <- kmeans(data_rec, centers = k, nstart = 1000)
```

```{r}
set.seed(123)
k <- 5
model <- kmeans(data, centers = k, nstart = 1000)
```

```{r}
centers <- model$centers
centers
```

```{r}
res_rec <- data.frame(names_rec,model_rec$cluster)
head(res_rec)
```

```{r}
res <- data.frame(names,model$cluster)
head(res)
```

We map the clustering results in which we can detect that southern European countries are grouped together with South American regions and Russia, pertaining to the second-best clan in terms of education. The strongest cluster encompasses Nordic economies, Australia, and North America whereas the least advanced one is occupied by most African economies in combination with India. 

```{r}
res$names[res$names == "United States"] <- "USA"
res$names[res$names == "United Kingdom"] <- "UK"
res$names[res$names == "Russian Federation"] <- "Russia"
map <- map_data("world") %>% 
  full_join(res, by =  c("region" = "names")) 

map %>% 
  ggplot(aes(long,lat))+
  geom_polygon(aes( group = group, fill = as.factor(model.cluster)))+
  theme_void()+
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())+
  labs(title = "World Clustering", fill = "Cluster Group")
```

```{r}
res_rec$names[res_rec$names == "United States"] <- "USA"
res_rec$names[res_rec$names == "United Kingdom"] <- "UK"
res_rec$names[res_rec$names == "Russian Federation"] <- "Russia"
map <- map_data("world") %>% 
  full_join(res_rec, by =  c("region" = "names")) 

map %>% 
  ggplot(aes(long,lat))+
  geom_polygon(aes( group = group, fill = as.factor(model_rec.cluster)))+
  theme_void()+
  theme(axis.title = element_blank(),
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())+
  labs(title = "World Clustering 2008-2018", fill = "Cluster Group")
```

## Select 10 random countries

We select 10 random economies from each cluster and compute the mean change over time in terms of population growth rate to input the information into our ARIMA series modelling. The primary requirement for our models is the stationarity of the data, which we will address by taking the first difference on each cluster and evaluating the optimal autoregressive and moving average components for each of our groups (Box and Jenkins, 1970). 

```{r}
set.seed(1234)
sample <- res %>% 
  group_by(model.cluster) %>%
  count() %>%
  dplyr::mutate(sample = list(sample(1:n,5,replace = FALSE))) %>% 
  unnest() %>% 
  dplyr::select(model.cluster, sample)
```

```{r}
new_res <- res %>% 
  arrange(desc(model.cluster)) %>% 
  group_by(model.cluster) %>% 
  mutate(id = 1:n()) %>% 
  ungroup() %>% 
  filter(!str_detect(names, "Income") & !str_detect(names, "Euro") & !str_detect(names, "income"), !str_detect(names, "countries"),!str_detect(names, "members"))

selection <- new_res %>% 
  inner_join(sample, by = c("id" = "sample", "model.cluster"))
  
```

## Time series Modelling

```{r}
data <- read.csv("final_data.csv")
```

We reload the data and filter for the relevant time period. Next, we have to select the corresponding time and cluster attributes we will use for the time series analysis. We will have to pivot the data in order to pipe this information into ARIMA function supplied by R:

```{r}
time_series <- data %>% 
  inner_join(selection, by = c("country_name" = "names")) %>% 
  filter(Year > 1979, Year <2022) %>% 
  dplyr::select(country_name,Year,model.cluster, Pop_growth)

```

```{r}
time_series <- time_series %>% 
  group_by(Year,model.cluster) %>% 
  mutate(growth = mean(Pop_growth)) %>% 
  dplyr::select(Year,model.cluster,growth) %>% 
  ungroup() %>% 
  distinct()
```

Check stationarity! The appraisal of the set of Dickey fuller tests (unit root) to evaluate the premise of stationarity in mean can be resorted to by the user in the appendix in which the null hypothesis states that the coefficient of such autoregressive component behaves like a random walk. 

```{r}
time_series %>% 
  ggplot()+
  aes(as.numeric(Year),growth, color = as.factor(model.cluster))+
  geom_line()+
  facet_wrap(~model.cluster, scales = "free")+
  theme(legend.position = "None")
```


The "AR" component will model the relationship between the current value of the temporal data and its past values and will assume that the latest observation of the series can be explained by a linear combination of its previous values. The "MA" element represents the association between the current measurement of the series and earlier shocks: 

\[ \dot{y}_t = c + \phi_1 \dot{y}_{t-1} + \phi_2 \dot{y}_{t-2} + \cdots + \theta_1 \varepsilon_t + \theta_2 \varepsilon_{t-1} + \cdots \]



```{r}
time_series <- time_series %>% 
  pivot_wider(names_from = model.cluster, values_from = growth)
```

Dickey fuller test (null is stationarity):

```{r}
adf.test(time_series$`1`)
adf.test(time_series$`2`)
adf.test(time_series$`3`)
adf.test(time_series$`4`)
adf.test(time_series$`5`)
```

```{r}
first_years <- time_series %>% 
  filter(as.numeric(Year) <= 2019)

last_years <- time_series %>% 
  filter(as.numeric(Year) >= 2019)

```

We generate a loop which iterates through our list of models and models the output for each of our clusters. The output is a list of models for the first years of data. By estimating a different model for each of our clusters, the results from our time series template serve as a basis in order to compare the quality of predictions we can obtain from our previous educational analysis. The results point to a very significant autoregressive component of population growth rate and, in the last 3 clusters, a very marked moving average ingredient driving population dynamics.

```{r}
model <- list()
for (i in 2:6) {
  new <- auto.arima(first_years[,i])
  model[[i-1]] <- new
}
```

```{r}
model
```

```{r}
cluster_1 <- data.frame(arima(first_years$`1`, order = c(2,0,0)) %>% 
  forecast(h = 3))
cluster_2 <- arima(first_years$`2`, order = c(2,1,0))%>% 
  forecast(h = 3) %>% 
  data.frame()
cluster_3 <- arima(first_years$`3`, order = c(2,1,1)) %>% 
  forecast(h = 3) %>% 
  data.frame()
cluster_4 <- arima(first_years$`4`, order = c(0,0,1)) %>% 
  forecast(h = 3) %>% 
  data.frame()
cluster_5 <- arima(first_years$`5`, order = c(1,1,2))%>% 
  forecast(h = 3) %>% 
  data.frame()

results <- data.frame(last_years,cluster_1$Point.Forecast,
           cluster_2$Point.Forecast,
           cluster_3$Point.Forecast,
           cluster_4$Point.Forecast,
           cluster_5$Point.Forecast) %>% 
  pivot_longer(-1, names_to = "Nature",values_to = "Value") %>% mutate(type = ifelse(str_detect(Nature,"cluster"),"Prediction","Original")) %>% 
  mutate(Cluster = str_extract(Nature,"\\d")) %>% 
  dplyr::select(Year,Cluster,type,Value)

results_2 <- data.frame(first_years) %>% 
  pivot_longer(-1, names_to = "Nature",values_to = "Value") %>% mutate(cluster = str_remove(Nature,"X")) %>% 
  mutate(Cluster = str_extract(Nature,"\\d")) %>% 
  dplyr::select(Year,Cluster,Value)
```

```{r}

results %>% 
  ggplot()+
  aes(as.numeric(Year),Value, color = type)+
  geom_line()+
  facet_wrap(~Cluster, ncol = 2, scales = "free_y")+
  theme_dark()+
  theme(legend.position = "None",
        plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank())+
  labs(title = "Time Series Plot",subtitle = "Red line accounts for original values and green for fitted values",
       y = "Population Growth", x = "Time")+
  geom_line(aes(as.numeric(Year),Value), data = results_2, color = "white")
  
```

We obtain accurate predictions such that the standard error of our model is around 0.55%, suggesting that, on average, the estimated population growth rate is expected to deviate from the true magnitude by approximately half a percentage point. . If the reader recalls, the standard error for the linear model was approximately 0.5%, indicating a high level of precision in predicting short-term average population trends through our education analysis compared to ordinary time series evaluation, which is often more appealing and widely favored. 

Standard error of the model: 

```{r}
 results %>% 
  pivot_wider(names_from = type,values_from = Value) %>% 
  mutate(sq_res = (Original-Prediction)^2,
         sd_err = (sum(sq_res)/15)^(1/2))
```
